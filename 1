import torch
from torch.functional import Tensor
import torch.nn as nn
from functools import partial
import math
import warnings
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import einsum
import numpy as np
from math import exp

class SlimmableConv2d(nn.Conv2d):
    def __init__(self, in_channels_list, out_channels_list, kernel_size=3, stride=1, padding=(1, 1), dilation=1, bias=False):
        super(SlimmableConv2d, self).__init__(in_channels_list, out_channels_list, kernel_size=3, stride=1, padding=(1, 1), dilation=1, bias=False)

        self.width_mul = 1
        self.in_channels_list = in_channels_list
        self.out_channels_list = out_channels_list

    def forward(self, input):
        if self.in_channels > 3:
            self.in_channels = int(self.in_channels_list * self.width_mul)
        self.out_channels = int(self.out_channels_list * self.width_mul)

        weight = self.weight[:self.out_channels, :self.in_channels, :, :]
        if self.bias is not None:
            bias = self.bias[:self.out_channels]
        else:
            bias = self.bias

        y = nn.functional.conv2d(input, weight, bias, self.stride, self.padding)
        return y


class SELayer(nn.Module):
    def __init__(self, channel):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(channel, channel, kernel_size=1, stride=1,padding=0, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channel, channel, kernel_size=1, stride=1,padding=0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x)#.view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return y * x


class Head(nn.Module):
    """ Head consisting of convolution layers
    Extract features from corrupted images, mapping N3HW images into NCHW feature map.
    """

    def __init__(self, in_channels, out_channels):
        super(Head, self).__init__()
        self.conv0 = nn.Conv2d(3, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.conv1 = GhostConv(out_channels, out_channels, stride=2)
        self.conv2 = GhostConv(out_channels, out_channels, stride=2)

    def forward(self, x):
        F0 = F.relu(self.conv0(x))
        F1 = F.relu(self.conv1(F0))
        F2 = F.relu(self.conv2(F1))

        return [F0, F1, F2]

class Tail(nn.Module):
    def __init__(self, in_channels):
        super(Tail, self).__init__()

        self.conv0 = GhostConv(in_channels*2, in_channels)
        self.conv1 = GhostConv(in_channels * 2, in_channels)
        self.conv2 = nn.Conv2d(in_channels, 3, kernel_size=3, stride=1, padding=1, bias=False)

    def forward(self, res,  x):
        [F0, F1, F2] = x
        out0 = F.relu(self.conv0(torch.cat((F1, F.interpolate(F2, F1.shape[2:], mode='bilinear')), 1)))
        out1 = F.relu(self.conv1(torch.cat((F0, F.interpolate(out0, F0.shape[2:], mode='bilinear')), 1)))
        out = self.conv2(out1)

        out =  out
        out = out.clamp(0, 1)
        return out, [out0, out1]

class GhostConv(nn.Module):
    def __init__(self, inchannels, channels,kernel_size=3, stride=1, padding=1):
        super(GhostConv, self).__init__()

        self.conv1 = nn.Conv2d(inchannels, channels // 2, kernel_size=kernel_size, stride=stride, padding=1*(kernel_size -1)//2, dilation=1, bias=False)
        self.conv2 = nn.Conv2d(channels // 2, channels // 4, kernel_size=kernel_size, stride=1, padding=1*(kernel_size -1)//2, dilation=1, bias=False)
        self.conv3 = nn.Conv2d(channels // 4, channels // 8, kernel_size=kernel_size, stride=1, padding=1*(kernel_size -1)//2, dilation=1, bias=False)
        self.conv4 = nn.Conv2d(channels // 8, channels // 8, kernel_size=kernel_size, stride=1, padding=1*(kernel_size -1)//2, dilation=1, bias=False)

        # self.conv1x1 = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = F.relu(self.conv2(x1))
        x3 = F.relu( self.conv3(x2))
        x4 = F.relu( self.conv4(x3))
        x = (torch.cat((x1, x2, x3, x4), 1))
        return x


class IFBlock(nn.Module):
    def __init__(self, inchannels, channels,kernel_size=3, stride=1, padding=1):
        super(IFBlock, self).__init__()

        self.conv1 = nn.Conv2d(inchannels, channels*4, kernel_size=1, stride=1, padding=0, bias=False)
        self.conv2 = nn.Conv2d(inchannels, channels*4, kernel_size=1, stride=1, padding=0, bias=False)

        self.conv3 = nn.Conv2d(inchannels*4, channels, kernel_size=1, stride=1, padding=0, bias=False)
        self.conv4 = nn.Conv2d(inchannels*4, channels, kernel_size=1, stride=1, padding=0, bias=False)

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.conv2(F.relu(x1))
        x3 = self.conv3(F.relu(x2))
        x4 = self.conv4(F.relu(x3))
        x = F.relu(torch.cat((x1, x2, x3, x4), 1))
        return x

class ResBlockWoAC(nn.Module):
    def __init__(self, inchannels, outchannels):
        super(ResBlockWoAC, self).__init__()

        self.conv1 = GhostConv(inchannels, outchannels)
        self.conv = GhostConv(outchannels, outchannels)

        self.se = SELayer(outchannels)

    def forward(self, x):
        residual = x
        out = F.relu(self.conv1(x))
        out = self.conv(out)
        out = self.se(out)
        out = residual + out

        return out


class RepHazeNetWAC(nn.Module):
    def __init__(self, channels=64, Nblks=6):
        super(RepHazeNetWAC, self).__init__()

        self.Blocks = nn.ModuleList([ResBlockWoAC(channels, channels) for _ in range(Nblks)])

        self.head = Head(channels, channels)
        self.tail = Tail(channels)

        # self.fusion = GhostConv(channels*Nblks,  channels, kernel_size=1,padding=0)
        # self.KD = nn.ModuleList([nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0, bias=False) for _ in range(6)])

    def forward(self, x):
        res = x
        [F0, F1, F2] = self.head(x)

        Fs = [F0, F1, F2]
        for i in range(len(self.Blocks)):
            F2 = self.Blocks[i](F2)
            Fs.append(F2)

        # F2 = self.fusion(torch.cat(Ft, 1))
        x, [out0, out1] = self.tail(res, [F0, F1, F2])
        Fs.append(out0)
        Fs.append(out1)
        return x, Fs
