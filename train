import argparse
import os
from os.path import join
import torch
from torch.utils.data import DataLoader
from dataset.dataITS import myDataloader
import scipy.misc
from network.loss import *
from torch.autograd import Variable
# from network.rD import define_D, GANLoss
from network.SSIM import SSIM
from adamW import AdamW
import random
import re
from scheduler import CyclicCosineDecayLR
import imageio
from network.dualR import cleaner
from network.UADN import *
import numpy as np
import torch.nn.functional as F
from torchvision import datasets
from network.KNNLSd import *
parser = argparse.ArgumentParser(description="PyTorch Train")
parser.add_argument("--batchSize", type=int, default=8, help="Training batch size")
parser.add_argument("--start_training_step", type=int, default=2, help="Training step")
parser.add_argument("--nEpochs", type=int, default=30, help="Number of epochs to train")
parser.add_argument("--lrG", type=float, default=1e-4, help="Learning rate, default=1e-4")
parser.add_argument("--lrD", type=float, default=1e-4, help="Learning rate, default=1e-4")
parser.add_argument("--step", type=int, default=10, help="Change the learning rate for every 30 epochs")
parser.add_argument("--start-epoch", type=int, default=1, help="Start epoch from 1")
parser.add_argument("--lr_decay", type=float, default=0.1, help="Decay scale of learning rate, default=0.5")
parser.add_argument("--resume", default="", type=str, help="Path to checkpoint (default: none)")
parser.add_argument("--scale", default=4, type=int, help="Scale factor, Default: 4")
parser.add_argument("--lambda_db", type=float, default=0.5, help="Weight of deblurring loss, default=0.5")
parser.add_argument("--gated", type=bool, default=True, help="Activated gate module")
parser.add_argument("--isTest", type=bool, default=False, help="Test or not")
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def mkdir_steptraing():
    root_folder = os.path.abspath('.')
    models_folder = join(root_folder, 'models/modelW')
    step1_folder, step2_folder, step3_folder = join(models_folder,'1'), join(models_folder,'2'), join(models_folder, '3')
    isexists = os.path.exists(step1_folder) and os.path.exists(step2_folder) and os.path.exists(step3_folder)
    if not isexists:
        os.makedirs(step1_folder)
        os.makedirs(step2_folder)
        os.makedirs(step3_folder)
        print("===> Step training models store in models/1 & /2 & /3.")


def mkdir_model(path):
    root_folder = os.path.abspath('.')
    models_folder = join(root_folder, path)
    isexists = os.path.exists(models_folder)
    if not isexists:
        os.makedirs(models_folder)

        print("===> Step training models store in models/1 & /2 & /3.")


def is_hdf5_file(filename):
    return any(filename.endswith(extension) for extension in [".h5"])


def print_network(net):
    if isinstance(net, list):
        net = net[0]
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print(net)
    print('Total number of parameters: %d' % num_params)


def which_trainingstep_epoch(resume):
    trainingstep = "".join(re.findall(r"\d", resume)[0])
    start_epoch = "".join(re.findall(r"\d", resume)[1:])
    return int(trainingstep), int(start_epoch)

import cv2
def DarkChannel(image, refine=True):
    r, g, b = cv2.split(image)
    dc = cv2.min(cv2.min(r, g), b)

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dark = cv2.erode(dc, kernel)

    if refine:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        r = 30
        eps = 1e-5
        dark = Guidedfilter(gray, dark, r, eps)

    return dark


def DarkCHannelTensor(tensor, refine=True):
    B, C, W, H = tensor.shape

    dark = torch.zeros(B, 1, W, H)
    for i in range(B):
        darki = tensor[i].clamp(0, 1)
        darki = darki.permute(1, 2, 0).detach().cpu().numpy()
        darki = DarkChannel(darki)
        # print(dFakeTi.shape)
        darki = torch.from_numpy(darki).unsqueeze(0).unsqueeze(1)
        dark[i] = darki
    return dark.cuda()

def Guidedfilter(im, p, r, eps):
    mean_I = cv2.boxFilter(im, cv2.CV_64F, (r, r))
    mean_p = cv2.boxFilter(p, cv2.CV_64F, (r, r))
    mean_Ip = cv2.boxFilter(im*p, cv2.CV_64F, (r, r))

    cov_Ip = mean_Ip - mean_I*mean_p

    mean_II = cv2.boxFilter(im*im, cv2.CV_64F, (r, r))
    var_I = mean_II - mean_I * mean_I

    a = cov_Ip / (var_I + eps)

    b = mean_p - a * mean_I

    mean_a = cv2.boxFilter(a, cv2.CV_64F, (r, r))
    mean_b = cv2.boxFilter(b, cv2.CV_64F, (r, r))

    q = mean_a * im + mean_b

    return q


class trainer_2:
    def __init__(self, train_gen, step=1, numD=4):
        super(trainer_2, self).__init__()

        self.numd = numD
        self.step = step
        self.trainloader = train_gen

        ## #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ##
        self.modelG = ULSN5().cuda()
        # self.modelG = torch.load('./models/ITSDnse/1//UADN_wLSD_M6N3_140.pkl')

        self.optimizer_G = torch.optim.Adam(self.modelG.parameters(), lr=1e-4, betas=(0.9, 0.999))
        self.scheduler_lrG = CyclicCosineDecayLR(self.optimizer_G, init_interval=40, min_lr=1e-6, restart_interval=40, restart_lr=1e-4)

        ## #### #### #### #### #### #### #### #### #### #### #### #### #### #### #### ##
        self.criterion = nn.L1Loss().to(device)
        self.VGGLoss = VGGLoss().to(device)
        # self.weight = [1./2**6, 1./2**5, 1./2**4, 1./2**3, 1./2**2, 1./2, 1]
        self.KNNLSD = KNNLSD().to(device)

    def opt_G1(self, Ix, GT, fake, conf, feas):
        self.optimizer_G.zero_grad()

        x_points = tensor2points(F.upsample_bilinear(GT, scale_factor=1/4))  # B, HW, C
        k_ind = find_k_neighbor(x_points, x_points, K=10)

        loss_KNNSD = 0
        g_loss_MSE1 = 0
        vggLoss = 0
        for i in range(len(conf)):
            GT0 = F.upsample_bilinear(GT, (fake[i].shape[2], fake[i].shape[3]))
            lam_cmp = 0.2

            tmp = torch.FloatTensor(1)
            tmp = Variable(tmp, False)

            with torch.no_grad():
                tmp.data[0] = -(torch.mean(torch.log(conf[i] + 1e-10)))
                tmp = tmp.cpu()
                if tmp.data[0] < 0.2:
                    lam_cmp = 0.09*lam_cmp * (np.exp(5.4 * tmp.data[0]) - 0.98)  # 0.09*lam_cmp/(np.exp(1.0*tmp.data[0]))
                    lam_cmp = lam_cmp.cuda()

            g_loss_MSE1 += (self.criterion(fake[i] * conf[i] + (1 - conf[i]) * GT0, GT0.detach())
                            - (torch.mean(torch.log(conf[i] + 1e-10))) * lam_cmp) * (0.8**(len(conf) - 1 - i))

            vggLoss += self.VGGLoss(fake[i], GT0.detach()) * (0.8**(len(conf) - 1 - i))

        for i in range(len(feas)):
            loss_KNNSD = loss_KNNSD + self.KNNLSD(feas[i], conf[i], k_ind.detach()) * (0.8**(len(conf) - 1 - i))

        loss = g_loss_MSE1 + vggLoss + loss_KNNSD*0.1

        loss.backward()
        self.optimizer_G.step()

        g_loss_MSE = self.criterion(fake[-1], GT.detach())
        return loss, g_loss_MSE, g_loss_MSE1
    def opt_G1woLSD(self, Ix, GT, fake, conf, feas):
        self.optimizer_G.zero_grad()

        g_loss_MSE1 = 0
        vggLoss = 0
        for i in range(len(conf)):

            GT0 = F.upsample_bilinear(GT, (fake[i].shape[2], fake[i].shape[3]))
            lam_cmp = 0.2

            tmp = torch.FloatTensor(1)
            tmp = Variable(tmp, False)

            with torch.no_grad():
                tmp.data[0] = -(torch.mean(torch.log(conf[i] + 1e-10)))
                tmp = tmp.cpu()
                if tmp.data[0] < 0.2:
                    lam_cmp = 0.09*lam_cmp * (np.exp(5.4 * tmp.data[0]) - 0.98)  # 0.09*lam_cmp/(np.exp(1.0*tmp.data[0]))
                    lam_cmp = lam_cmp.cuda()

            g_loss_MSE1 += (self.criterion(fake[i] * conf[i] + (1 - conf[i]) * GT0, GT0.detach())
                            - (torch.mean(torch.log(conf[i] + 1e-10))) * lam_cmp) * (0.8**(len(conf) - 1 - i))

            vggLoss += self.VGGLoss(fake[i], GT0.detach()) * (0.8**(len(conf) - 1 - i))

        # vggLoss = self.VGGLoss(fake[-1], GT.detach())
        loss = (g_loss_MSE1 + vggLoss)

        loss.backward()
        self.optimizer_G.step()

        g_loss_MSE = self.criterion(fake[-1], GT.detach())
        return loss, g_loss_MSE, g_loss_MSE1

    def checkpoint(self, epoch):
        path = "models/ITSDnse/{}/".format(1)
        mkdir_model(path)
        model_out_path = path + "/UADN_wLSD_M6N3_{}.pkl".format(epoch)
        torch.save(self.modelG, model_out_path)

        print("===>Checkpoint saved to {}".format(model_out_path))

    def train(self, epoch, train_gen):
        self.trainloader = train_gen

        self.scheduler_lrG.step(epoch=epoch-1)

        print(self.optimizer_G.param_groups[0]["lr"])

        epoch_loss1 = 0
        epoch_loss2 = 0
        epoch_loss3 = 0
        for iteration, (Ix, Jx) in enumerate(self.trainloader):
            Ix = Ix.to(device)
            Jx = Jx.to(device)

            fake, conf, feas = self.modelG(Ix)

            loss1, loss2, loss3 = self.opt_G1(Ix, Jx, fake, conf, feas)
            # loss1, loss2, loss3 = self.opt_G1woLSD(Ix, Jx, fake, conf, feas)
            epoch_loss1 += float(loss1)
            epoch_loss2 += float(loss2)
            epoch_loss3 += float(loss3)

            if iteration % 100 == 0:
                print(
                    "===> Epoch[{}]({}/{}): Loss{:.4f};".format(epoch, iteration, len(trainloader), loss2.cpu()))

                Ix_cc = fake[-1]
                Ix_cc = Ix_cc.clamp(0, 1)
                fake = Ix_cc[0].permute(1, 2, 0).detach().cpu().numpy()

                fake1 = conf[-1]
                fake1 = fake1.clamp(0, 1)
                fake1 = fake1[0].permute(1, 2, 0).detach().cpu().numpy()
                confT = np.tile(fake1, (1, 3))
                # Ix_cc = np.hstack([fake1, Ix_cc]

                Ix = Ix.clamp(0, 1)
                Ix = Ix[0].permute(1, 2, 0).detach().cpu().numpy()
                # Ix_cc = np.hstack([Ix, Ix_cc])

                Jx = Jx.clamp(0, 1)
                Jx = Jx[0].permute(1, 2, 0).detach().cpu().numpy()
                Ix_cc = np.hstack([Ix, confT, fake, Jx])

                #print(Ix_cc.shape)
                imageio.imsave('./results' + '/' + str((epoch - 1) * 100 + iteration / 100) + '.png', np.uint8(Ix_cc*255))

                print("MSE:{:4f},MSSIM:{:4f},VGG:{:4f},VGG:{:4f}".format(loss1, loss2, loss3, loss1))

        print("===>Epoch{} Complete: Avg loss is :Loss1:{:4f},Loss2:{:4f},Loss3:{:4f},  ".format(epoch, epoch_loss1 / len(trainloader), epoch_loss2 / len(trainloader), epoch_loss3 / len(trainloader)))

        path = './log/logITS.txt'
        with open(path, 'a') as f:
            f.write("===>Epoch{} Complete: Avg loss is :Loss1:{:4f},Loss2:{:4f},Loss3:{:4f} \n ".format(epoch, epoch_loss1 / len(trainloader), epoch_loss2 / len(trainloader), epoch_loss3 / len(trainloader)))


opt = parser.parse_args()
opt.seed = random.randint(1, 10000)
torch.manual_seed(opt.seed)
torch.cuda.manual_seed(opt.seed)
trainloader = myDataloader().getLoader()

for i in range(1, 2):
    print("===> Loading model and criterion")

    trainModel = trainer_2(trainloader, step=1, numD=1)

    for epoch in range(1, 320):

        print("epoch {}:-------------------------------".format(epoch))
        trainModel.train(epoch, trainloader)
        trainModel.checkpoint(epoch)
